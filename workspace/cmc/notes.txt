= 2011-07-20 =

Prepare cmc corpus
	# Set up corpus: 
	copy documents and annotations into corpus_doc tables
	would like to use same format for both i2b2 and cmc corpora
	run data/corpus_doc.sql
	
	# Set up folds: FoldGeneratorImpl on data/corpus.properties.xml
	
	# Set up anno_contain table
	run data/insert_anno_contain.sql
	
	# Try 'Sujeevan' kernel
	in libsvm/sujeevan/beans-corpus-kernel.xml
	doc -> section -> named entities -> concepts
	               -> words
	Use outermost named entities - named entities contained within other named entities

	# Generate concept graph
	Run ConceptDaoImpl on data/corpus.properties.xml
	
	# Store infocontent
	Run InfoContentEvaluatorImpl on data/corpus.properties.xml
	
	# Modified beans-kernel-sim.xml to use placeholders for conceptGraphName and corpusName
	need to verify that this doesn't break anything
	
	# performance abominable! go back to basics
	
= 2011-07-20 =
Recreate sujeevan's results
* restore original concept graph (cmcorig)
* compared concepts - many concepts in suj's annoations that are missing in ctakes annotations -> todo: reannotate with full umls
* use sujeevans annotations (suj_concept, suj_lexical_unit ...)
* recomputed infocontent - ok - got same results

Modify Sujeevan's code to
* store document annotations in database - that way we can see if our kernel reproduces results with his anno's
* what we were missing were the section that the lexical units belonged to - modify to store this info
* store kernel evaluations in database

Why the difference in the concept graphs?
* umlspar - new concept graph
* cmcorig - old concept graph
* the old concept graph is much larger than the new one
* only difference is I now cut out forbidden concepts
* maybe umls on desktop different?
* umls on laptop & ristretto identical: laptop has fewer atoms in MRCUI.  But same SABS - difference = Languages - ristretto has SPA also.

= 2011-09-08 =
Continue recreate suj's results, experiment=sujeevan
export data:
{{{
ant -Dexport.dir=libsvm/sujeevan kernel.export.gram > ant.out
}}}
run cross-validation:
{{{
nohup ant -Dytex.home=/vol/Users/vng2/ytexWorkspace -Dkernel.data=/vol/Users/vng2/ytexWorkspace/cmc/libsvm/sujeevan process.dir > ant.out 2>&1 &
}}}

Cross-validation results (/cmc/data/cv_eval.sql)
macro-f1: 0.49
micro-f1: 0.79

Get best costs using query in cv_eval.sql
Export data for test using:
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dtest.all.target=test.export.label -Dkernel.experiment=sujeevan-test test.all > ant.out 2>&1 &
}}}
Process dir
{{{
cd ${YTEX_HOME}/ytex.kernel/scripts
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=sujeevan-test -Dkernel.exp.base=${YTEX_HOME}/cmc/libsvm/sujeevan-test  iter.dirs > ant.out 2>&1 &
}}}

Test results, micro:
experiment	ppv	sens	f1
sujeevan-test	0.8598	0.7535	0.803147957602
sujeevan-test2	0.7890	0.7602	0.774332300542 (using costs from best run with cmc-suj)

http://computationalmedicine.org/challenge/previous
Aseervatham: 0.8498 micro f1, 0.6756 macro f1

From previous run (), micro:
experiment	ppv	sens	f1
cmc-suj-test	0.932755985736118	0.873152122079161	0.901970443349754
cmc-test	0.869959677419355	0.716182572614108	0.785616750113792
ctakes-tuifilter-test	0.934938524590164	0.858015984955336	0.894827163520471
{{{
select *, 2*ppv*sens/(ppv+sens)
from
(
select tp/(tp+fp) ppv, tp/(tp+fn) sens
from
(
select sum(num_true_positives) tp, sum(num_false_positives) fp, sum(num_true_negatives) tn, sum(num_false_negatives) fn
from weka_results 
where experiment = 'cmc-suj-test'
) s
)s
;
}}}

Why the difference?  Looking at label 1, we have a max f1 of 0 on cross validation with new setup, old setup was 0.87.
Not clear what's going on - possibly the kernel evaluations are now different than before

= 2011-09-09 =
Rerun ctakes-tuifilter
Redone with kernel experiment kern-ctakes, tree tree-ctakes

Structure and kernels
instance                   [norm [pow [convolution]]]
 |
 |--document               [norm [section identical check] * [convolution]]
 		|
 		|-- word           [identity kernel]
 		|
 		|-- named entity   [convolution]
 		      |
 		      |-- concept  [concept kernel : tuifilter * lin * lch]
* generate the tree	      
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.tree=tree-ctakes kernel.tree > ant.out 2>&1 &
}}}
* compute the kernel with the original cmc concept graph (specified in ytex.properties)
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.evalTest=true -Dkernel.tree=tree-ctakes -Dkernel.experiment=kern-ctakes kernel.eval > ant.out 2>&1 &
}}}
* export gram matrices for libsvm cv
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes kernel.export > ant.out 2>&1 &
}}}
* run cv
{{{
cd ${YTEX_HOME}/ytex.kernel/scripts
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes -Dkernel.data=${YTEX_HOME}/cmc/libsvm/kern-ctakes process.dir > ant.out 2>&1 &
}}}

CV Results
macro f1: 0.52
micro ppv/sens/f1: 0.8822	0.7419	0.805990000616

* run on test set - export data and run libsvm
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-test test.all > ant.out 2>&1 &
}}}
in the ballpark - 0.8370 vs 0.8498
ppv	sens	f1
0.8943	0.7867	0.837056287924

*** oops ***
the infocontent had not been calculated with ctakes annotations!
The concept kernel (lin) was returning 0 for all concept evaluations - that's why it went so fast?
anyways, we achieved near sujeevan performance without a concept kernel - just a convolution kernel

Next step - repeat with infocontent computed for ctakes annotations
Also todo: get rid of concept kernel and use identity kernel

== kern-ctakes-ic ==
* run infocontent, compute kernel
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.evalTest=true -Dkernel.tree=tree-ctakes -Dkernel.experiment=kern-ctakes-ic infocontent kernel.eval > ant.out 2>&1 &
}}}
* export gram matrices:
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-ic kernel.export > ant.out 2>&1 &
}}}
* run libsvm cv:
{{{
cd ${YTEX_HOME}/ytex.kernel/scripts
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-ic -Dkernel.data=${YTEX_HOME}/cmc/libsvm/kern-ctakes-ic process.dir > ant.out 2>&1 &
}}}

Cross-validation results (/cmc/data/cv_eval.sql)
macro-f1: 0.52
micro-ppv/sens/f1: 0.8797	0.7513	0.810445873697

== kern-ctakes-ic-test ==
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-ic-test test.all > ant.out 2>&1 &
}}}
in the ballpark, but actually lower!
micro ppv/sens/f1: 0.8915	0.7842	0.834414632691

== kern-ctakes-umlspar ==
kern-ctakes-ic was done with the original concept graph that contained illegal concepts.
recompute kernel with new concept graph using all of umls and the same edges (REL = PAR and RELA = inverse_isa).
* modify ytex.properties - use umlspar concept graph
* run infocontent, compute kernel
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.evalTest=true -Dkernel.tree=tree-ctakes -Dkernel.experiment=kern-ctakes-umlspar  kernel.eval > ant.out 2>&1 &
}}}
* export gram matrices:
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-umlspar kernel.export > ant.out 2>&1 &
}}}
* run libsvm cv:
{{{
cd ${YTEX_HOME}/ytex.kernel/scripts
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-umlspar -Dkernel.data=${YTEX_HOME}/cmc/libsvm/kern-ctakes-ic process.dir > ant.out 2>&1 &
}}}

basically same as with cmcorig:
macro f1: 0.52
micro ppv/sens/f1: 0.8788	0.7476	0.807908116085

== kern-ctakes-umlspar-test ==

{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-umlspar-test test.all > ant.out 2>&1 &
}}}

* results - even lower than with cmcorig!!!
micro ppv/sens/f1: 0.8872	0.7768	0.828337692308

== kern-ctakes-flatne 9/12 ==
previous tree consolidated outermost ne's with the inner ne's.  Try separating them - this is what we did previously.
* set in ytex.properties
{{{
tree.name=tree-ctakes-flatne 
}}}
* run cross validation
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-flatne kernel.tree cv.all > ant.out 2>&1 &
}}}

macro: 0.514945955042806
micro: 0.8778	0.7451	0.806024745825

* test - a little lower than kern-ctakes, better than kern-ctakes-ic and kern-ctakes-umlspar
micro ppv/sens/f1: 0.8955	0.7826	0.835252130386

* try with weights
	** create cmc/exp/classWeights.properties, add property to ytex.properties
	** delete all label20* files because this has balanced classes
	** run cv.eval
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-flatne cv.eval > ant.out 2>&1 &
}}}
	** generate best params
	** run test:
{{{	
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-flatne-test test.all > ant.out 2>&1 &
}}}	
worse than the unweighted version
micro ppv/sens/f1: 0.8825	0.7851	0.830955564884
macro: 0.40

== ctakes-tuifilter-test 9/12 ==
Why/how was this so much better than the new stuff?
Looked at the trees - identical except that cuis/words can appear multiple times.
Reran libsvm on the data files, using optimal properties from that run. 
Results almost identical to the new results:
micro ppv/sens/recall: 0.8787	0.7876	0.830659689132

So, either we are calculating the results incorrectly now, or we were calculating the results incorrectly then.
or the new libsvm is different.
TODO: import instance-level predictions and verify.

== ctakes-tuifilter-test2 9/13 ==
Used models from back then and did the predictions.
Results slightly worse: 
micro ppv/sens/recall: 0.8773	0.7776	0.824446770198

== ctakes-tuifilter-test3 9/13 ==
Loaded previous predictions into database, results identical to ctakes-tuifilter-test2:
micro ppv/sens/recall: 0.8773	0.7776	0.824446770198

= Summary 9/13 =
kern-ctakes test marginally better than all others.  Interesting: * does not use semantic similarity! *
kern-ctakes-ic-test 2nd best, interestingly: * uses old conceptgraph that contains illegal concepts! *
kern-ctakes-flatne-test 3rd best, interestingly: only one that can be easily reproduced - uses current concept graph
Old test (ctakes-tuifilter-*): apparently miscalculated IR stats previously

Query:
{{{
select *, 2*ppv*sens/(ppv+sens) f1
from
(
	select experiment, tp/(tp+fp) ppv, tp/(tp+fn) sens
	from
	(
		select experiment, sum(tp) tp, sum(fp) fp, sum(tn) tn, sum(fn) fn
		from classifier_eval e
		inner join classifier_eval_ir i on e.classifier_eval_id = i.classifier_eval_id and i.ir_class_id = 1
		where experiment like '%-test%'
        and name = 'cmc.2007'
    group by experiment
	) s
)s
order by 2*ppv*sens/(ppv+sens) desc
}}}

results:
experiment	ppv	sens	f1
kern-ctakes-test	0.8943	0.7867	0.837056287924
kern-ctakes-ic-test	0.8915	0.7842	0.834414632691
kern-ctakes-flatne-test	0.8825	0.7851	0.830955564884
ctakes-tuifilter-test	0.8787	0.7876	0.830659689132
kern-ctakes-umlspar-test	0.8872	0.7768	0.828337692308
ctakes-tuifilter-test3	0.8773	0.7776	0.824446770198
ctakes-tuifilter-test2	0.8773	0.7776	0.824446770198
sujeevan-test	0.8598	0.7567	0.804962152799
sujeevan-test2	0.7890	0.7602	0.774332300542 

Next steps: 
* try kern-ctakes-flatne without semantic similarity - just convolution kernel
* try with polynomial mixing kernel
* try with semi-supervised learning

== kern-ctakes-poly 9/13 ==
This is with the polynomial mixing kernel.  
Actually didn't bother with libsvm - the results are identical to the other kernels - the only difference is 
due to finite precision rounding errors.  These are identical with uniform weights - with the polynomial 
mixing kernel we divide by the sum of the weights, with the convolution kernel we don't do that.  However,
when we take the norm, we get the same result out of it (divide by constant corresponding to length).

== kern-ctakes-ident ==

* cross validation:
{{{
cd ${YTEX_HOME}/cmc
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-ident cv.all > ant.out 2>&1 &
}}}

macro f1: 0.506444444444444
micro: 0.8831	0.7724	0.824048855331

* test:
{{{	
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-ident-test test.all > ant.out 2>&1 &
}}}	
This is the best result; interesting as it does not use any semantic similarity.
macro: 0.4387
micro: 0.8900	0.7925	0.838424962853

== semi-supervised 9/13 ==
* svm: for svm solutions, need a primal representation.  
can convert to primal using kpca, or mds; very easy to do in R w/ kernlab.
However, outputting the data is very slow, not clear why.

* manifold: can convert kernel into distance metric.  
however need to compute kernel for all instances (train & test).
would be easy to use nystrom approximation for test instances.

* plan: start with manifold - straight forward to output distance matrices.  
Then, move on to svm-based solutions.

* compute the kernel for all pairs of instances: modify kernel.xml and specify all instances as training instances.
change query in kernel.xml to:
{{{
select doc_id, 1 
from corpus_doc 
where corpus_name = 'cmc.2007'
}}}

* eval kernel:
{{{
nohup ant -Dytex.home=${YTEX_HOME} -Dkernel.experiment=kern-ctakes-ident kernel.eval > ant.out 2>&1 &
}}}

== semil results ==
much worse:
test macro f1: 0.3386398355159
test micro ppv/sens/f1: 0.7891	0.6830	0.732226479179
However - for many classes semiL is much higher.
TODO: select best method by cross validation, use best method for final test eval. 

== svmlight results ==
* export cv data
mysql --user=ytex --host=ristretto.med.yale.edu ytex < instance.sql > instance.txt
cd to svmlight/kern-ctakes-ident
source("svmlight.R")
export(gramFile = "../../semiL/kern-ctakes-ident/data.txt", 
	idFile = "../../semiL/kern-ctakes-ident/instance_id.txt", 
	features=100)
* way too slow - computer crashed after a couple hours...

== libsvm w/ kpca ==
verify that libsvm works properly with kpca'ed representation before testing svmlin
* export instance data
mysql -u ytex -p ytex -h ristretto.med.yale.edu < instance.sql > instance.txt
* run to generate libsvm cv files:
when using all features get a wide, dense matrix - takes too long to write out to disk!
take top 200 features
{{{
source("svmlight.R")
export(gramFile = "semiL/kern-ctakes-ident/data.txt", 
	idFile = "semiL/kern-ctakes-ident/instance_id.txt", 
	instanceFile = "libsvm/kpca/instance.txt", 
	outputDir= "libsvm/kpca/",
	features=200)
}}}
* run cross validation
micro ppv/sens/f1: 0.8866	0.7103	0.788718116350

* test
{{{
export(gramFile = "semiL/kern-ctakes-ident/data.txt", 
	idFile = "semiL/kern-ctakes-ident/instance_id.txt", 
	instanceFile = "libsvm/kpca-test/instance.txt", 
	outputDir= "libsvm/kpca-test/",
	features=200)
}}}

== svmlin w/kpca ==
query to get positive classes
{{{
select cast(concat('kernel.classrel.', label_id, '=', pos/neg) as char(100))
from
(
    select l.label_id, sum(case when a.class = 1 then 1 else 0 end) pos, sum(case when a.class = 0 then 1 else 0 end) neg
    from corpus_doc d
    inner join corpus_doc_anno a on a.corpus_name = d.corpus_name and a.doc_id = d.doc_id
    inner join corpus_label l on l.corpus_name = d.corpus_name and l.label = a.label
    group by label_id
) s
}}}
macro: 0.41
micro: 0.7149	0.7826	0.747219686144
but outperformed libsvm on many classes

== best of the best ==
micro ppv/sens/f1: 0.8606, 0.8199, 0.839757143707
select *, 2*ppv*sens/(ppv+sens)
from
(
	select tp/(tp+fp) ppv, tp/(tp+fn) sens
	from
	(
		select sum(tp) tp, sum(fp) fp, sum(tn) tn, sum(fn) fn
		from
		(
		    select e.experiment, e.label, i.*
		    from classifier_eval e
		    inner join classifier_eval_ir i on e.classifier_eval_id = i.classifier_eval_id and 
		        (
		            (experiment not like '%semil%' and i.ir_class_id = 1)  
		            or
		            (experiment like '%semil%' and i.ir_class_id = 2)
		        )
		    inner join
		    (
		    select e.label, 
		        case 
		            when min(e.exp) = 1 then 'kern-ctakes-ident-test'
		            when min(e.exp) = 2 then 'kern-ctakes-ident-semil-test'
		            else '10kpca-svmlin-test'
		        end experiment
		    from
		    (
		            select experiment, label, i.*,
		            case
		                when experiment = 'kern-ctakes-ident-semil-test' then 2
		                when experiment = 'kern-ctakes-ident-test' then 1
		                else 3
		            end exp
		            from classifier_eval e
		            inner join classifier_eval_ir i on e.classifier_eval_id = i.classifier_eval_id and 
		                (
		                    (experiment not like '%semil%' and i.ir_class_id = 1)  
		                    or
		                    (experiment like '%semil%' and i.ir_class_id = 2)
		                )
		        where experiment in ('kern-ctakes-ident-semil-test', 'kern-ctakes-ident-test', '10kpca-svmlin-test')
		    ) e inner join
		    (
		            select label, max(f1) f1
		            from classifier_eval e
		            inner join classifier_eval_ir i on e.classifier_eval_id = i.classifier_eval_id and 
		                (
		                    (experiment not like '%semil%' and i.ir_class_id = 1)  
		                    or
		                    (experiment like '%semil%' and i.ir_class_id = 2)
		                )
		        where experiment in ('kern-ctakes-ident-semil-test', 'kern-ctakes-ident-test', '10kpca-svmlin-test')
		        group by label
		    ) s on e.label = s.label and e.f1 = s.f1
		    group by label
		    ) l on l.label = e.label and l.experiment = e.experiment
		)s
	)s
)s
;

== svmlin ==
ok, go back to basics - cross validation to get best params.
maybe the cost param is important.  
test transductive svm with various lambdaW.  
use a low lambdaU to speed things up.
only trained on labels with > 2 training examples
del label12*
del label19*
del label21*
del label25*
del label26*
del label28*
del label32*
del label37*
del label41*
del label42*

== svmlin 9/21 ==
looked at cross-validation results.
svmlin performed (much) better on labels with fewer training examples.
this is not going to help micro-averaged f1 that much, but shows that semi-supervised does better with fewer training examples.

TODO: with labels 20 & 9 (most positive examples) test svmlin vs. libsvm 
with varying number of training examples.

TODO: retest svmlin with lambda parameters determined from cross-validation

TODO: take a closer look at semil
